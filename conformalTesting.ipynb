{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from ast import literal_eval\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# The batch size\n",
    "batch_size = 512 \n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True,\n",
    "    transform=transform_train,\n",
    ")\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "trains_set, validation_set = random_split(train_set, [0.75, 0.25])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation_set, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, conv1_hidden=6, conv2_hidden=16, conv_kernel_size=5, pool_kernel_size=2, pool_stride=2, linear1_hidden=120, linear2_hidden=84):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, conv1_hidden, conv_kernel_size)\n",
    "        self.pool = nn.MaxPool2d(pool_kernel_size, pool_stride)\n",
    "        self.conv2 = nn.Conv2d(conv1_hidden, conv2_hidden, conv_kernel_size)\n",
    "        self.fc1 = nn.Linear(conv2_hidden * 5 * 5, linear1_hidden)\n",
    "        self.fc2 = nn.Linear(linear1_hidden, linear2_hidden)\n",
    "        self.fc3 = nn.Linear(linear2_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        intermediate_outputs.append(x)\n",
    "        x = self.pool(F.relu(x))\n",
    "        intermediate_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        intermediate_outputs.append(x)\n",
    "        x = self.pool(F.relu(x))\n",
    "        intermediate_outputs.append(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        intermediate_outputs.append(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        intermediate_outputs.append(x)\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        intermediate_outputs.append(x)\n",
    "\n",
    "        return x, intermediate_outputs\n",
    "    \n",
    "cnn = CNN().to(device)\n",
    "optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer_cnn.zero_grad()\n",
    "        outputs, intermediate_outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs, intermediate_outputs = cnn(inputs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.detach().numpy()\n",
    "outputs = outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"labels.csv\", \"w\")\n",
    "f.write(\"label;output\\n\")\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels_str = labels[i]\n",
    "    outputs_str = str(outputs[i])\n",
    "    # remove \\n from string\n",
    "    outputs_str = outputs_str.replace(\"\\n\", \"\")\n",
    "    f.write(str(labels_str) +\";\"+ str(outputs_str) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# we have to remember to turn \" \" into \",\"\n",
    "\n",
    "data1 = pd.read_csv('labels (2).csv', sep=';')\n",
    "\n",
    "smx = data1['output']\n",
    "labels = data1['label'].astype(int)\n",
    "\n",
    "# create an empty array\n",
    "lista = np.array([])\n",
    "for i in range(len(smx)):\n",
    "    lista = np.append(lista, literal_eval(smx[i]))\n",
    "    \n",
    "smx = lista.reshape(len(smx), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 10)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "n=200 # number of calibration points\n",
    "alpha = 0.09 # 1-alpha is the desired coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "\n",
    "# n Trues e (smx.shape[0]-n) Falses\n",
    "idx = np.array([1] * n + [0] * (smx.shape[0]-n)) > 0 \n",
    "\n",
    "# embaralha os Trues e Falses\n",
    "#np.random.shuffle(idx) \n",
    "\n",
    "# pega os valores de softmax de acordo com os Trues e Falses\n",
    "cal_smx, val_smx = smx[idx,:], smx[~idx, :] \n",
    "val_smx = val_smx[2].reshape(1,10)\n",
    "\n",
    "# pega os valores de labels de acordo com os Trues e Falses\n",
    "cal_labels, val_labels = np.array(labels[idx]), np.array(labels[~idx])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 10), (1, 10))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_smx.shape, val_smx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), 7)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_labels.shape, val_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal prediction happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: get conformal scores. n = calib_Y.shape[0]\n",
    "\n",
    "# criamos um vetor cal_scores com (1 -(probabilidade atribuida pelo modelo de que a imagem tenha seu label verdadeiro))\n",
    "# cal score é quanto menor, melhor\n",
    "cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "\n",
    "# 2: get adjusted quantile\n",
    "# qhat será o valor de s_i (entrada de cal_scores) que limita os 1-alpha menores scores (os melhores!)\n",
    "q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "qhat = np.quantile(cal_scores, q_level, interpolation='higher')\n",
    "\n",
    "prediction_sets = val_smx >= (1-qhat) # 3: form prediction sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical coverage is: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels].mean()\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False,  True, False,\n",
       "        False]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
